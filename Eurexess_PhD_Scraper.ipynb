{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euraxess scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the search_wrds by the domain yoiu want\n",
    "search_wrds = ['bioinformatics','computational biology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pages to retrieve for each search word\n",
    "pages = range(20)\n",
    "domain = 'phd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ids(search_1,page):\n",
    "    # Set the URL you want to webscrape from\n",
    "    if page ==0:\n",
    "        url = 'https://euraxess.ec.europa.eu/site/search/type/offer_posting?keywords='+search_1+'%20'+domain+'&sort=changed&order=desc'\n",
    "    else:   \n",
    "        url = 'https://euraxess.ec.europa.eu/site/search/type/offer_posting?keywords='+search_1+'%20'+domain+'&sort=changed&order=desc&page='+str(page)\n",
    "    # Connect to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse HTML and save to BeautifulSoup object¶\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    ids = soup.findAll('h2')[3:]\n",
    "    ids = [int(str(id_).split('/jobs/')[1].split('\"')[0]) for id_ in ids]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_metadata(url_):\n",
    "    response_ = requests.get(url_)\n",
    "    soup_ = BeautifulSoup(response_.text, \"html.parser\")\n",
    "    x = ' '.join(soup_.html.findAll(text=True,recursive=True)) \n",
    "    #Position Name\n",
    "    pos_name = x.split('Jobs & Funding  ›')[1].split('\\n \\n \\n')[0][1:-1]\n",
    "    # Deadline\n",
    "    deadline = str(soup_.findAll('h4')[1]).split('Deadline: ')[1].split(' -')[0]\n",
    "    day = deadline.split(' ')[0]\n",
    "    time = deadline.split(' ')[1]\n",
    "    # Institute\n",
    "    y = x.split('ORGANISATION/COMPANY')\n",
    "    institute = y[1].split(' \\n ')[2][5:-6]\n",
    "    # loaction\n",
    "    y = x.split('LOCATION')\n",
    "    location = y[1].split(' \\n ')[2][5:-6]\n",
    "    country = location.split(' › ')[0]\n",
    "    if location.count('›') ==1:\n",
    "        city = location.split(' › ')[1]\n",
    "    else:\n",
    "        city = ' '\n",
    "    # determine job describtion\n",
    "    describtion = x.split('\\n \\n \\n \\n \\n \\n \\n \\n')[19]#.split('EURAXESS offer ID')[0]#.split('\\n \\n \\n \\n \\n \\n \\n \\n')[1]\n",
    "    #starting date\n",
    "    if x.count('OFFER STARTING DATE') >0:\n",
    "        y = x.split('OFFER STARTING DATE')\n",
    "        offering_date = y[1].split('\\n')[2]\n",
    "    else:\n",
    "        offering_date = 'UNDEFINED'\n",
    "    #full text\n",
    "    html = str(x)\n",
    "    return pos_name,day, time, institute, country,city,offering_date,describtion,html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_job_datafarme(search_wrds):\n",
    "    columns = ['Euraxess_Link','Name','Deadline_day','Deadline_time','institute','country','city','offering_date',\n",
    "               'describtion','html','search']\n",
    "    final_df = pd.DataFrame(columns= columns)\n",
    "    for search_term in search_wrds:\n",
    "        search_1 = search_term.replace(' ', '%20') \n",
    "        i = 0        \n",
    "        df = pd.DataFrame(columns= columns,index=range(10*len(pages)))\n",
    "        df['search'] = search_term\n",
    "        for page in pages:\n",
    "            ids = extract_ids(search_1,page)\n",
    "            \n",
    "            for id_ in ids:\n",
    "                url_ = 'https://euraxess.ec.europa.eu/jobs/'+str(id_)\n",
    "                df.loc[i,'Euraxess_Link'] = url_\n",
    "                df.loc[i,['Name','Deadline_day','Deadline_time','institute','country','city','offering_date'\n",
    "                          ,'describtion','html']] = extract_job_metadata(url_)\n",
    "                i +=1\n",
    "        final_df = pd.concat([final_df,df])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = build_job_datafarme(search_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_ = final_df.drop_duplicates(subset=['Euraxess_Link','Name','Deadline_day','Deadline_time',\n",
    "                                             'institute','country','city','describtion','html'])\n",
    "from datetime import datetime\n",
    "#final_df_['day'] = pd.to_datetime(final_df_['day'],format='%d/%m/%y')\n",
    "final_df_.day = final_df_.day.apply(lambda x: datetime.strptime(x, \"%d/%m/%Y\"))\n",
    "final_df_= final_df_.sort_index(by=['Deadline_day'])\n",
    "final_df_.to_csv('PhD_Scraping.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
